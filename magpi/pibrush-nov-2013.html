<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<!-- AUTOGENERATED FILE: ALL EDITS WILL BE LOST!!! -->
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="cow">
<title>PiBrush &mdash; Painting with the Pi and XLoBorg</title>
<style type="text/css">
/*<![CDATA[*/
<!--
html, body {
  background: #fff;
  color: #000;
  font-family: sans-serif;
}
h1,h2,h3,h4,h5,p,ul,ol { font-family: sans-serif; }
pre, pre ol, pre ul { font-family: monospace; }
h3.navhead {
  font-size: 100%;
}
div.banner {
  border: none;
  margin-right: 0px;
  margin-left: 0px;
  padding: 0.09em;
  text-align: center;
  font-weight: bold; 
}
div.banner a:link, div.banner {
  background: #A0D0F0;
  color: #000000;
}
div.banner a:active {
  background: #000000;
  color: #FFFFFF;
}
div.banner a:hover {
  background: #000000;
  color: #FFFFFF;
-->
/*]]>*/
</style>
</head>
<body bgcolor="#FFFFFF" text="#000000">
<h1 class="cow-title-heading">PiBrush &mdash; Painting with the Pi and XLoBorg</h1>

<p><em>Turning an accelerometer into a paint brush with the Raspberry Pi</em></p>
<!--% %meta(summary=Turning an accelerometer into a paint brush with Raspberry Pi)-->


<p><em><a class="cow-url" href="http://pi.gate.ac.uk/pages/about.html#fred">Fred Sonnenwald</a> and
<a class="cow-url" href="http://pi.gate.ac.uk/pages/about.html#hamish">Hamish Cunningham</a>, November
2013</em></p>

<p><hr></p><h1 class="cow-heading">About the authors</h1>

<table> <tr><td>
<img class="cow-img" src="images/fred.jpg" alt="Fred" width="150">
</td><td>
<p><a class="cow-url" href="http://sonnenwald.co.uk">Fred</a>
is currently completing a PhD in Civil Engineering at the
University of Sheffield.</p>
</td></tr>
</table>

<table> <tr><td>
<img class="cow-img" src="images/hamish-headshot.jpg" alt="Hamish" width="150">
</td><td>
<p><a class="cow-url" href="http://gate.ac.uk/hamish/">Hamish</a> is a Research
Professor in Computer Science 
in Sheffield currently working on various Pi-related projects at
<a class="cow-url" href="http://Pi.GATE.ac.uk/">http://Pi.GATE.ac.uk/</a></p>
</td></tr>
</table>

<p><b>Thanks</b> to <a class="cow-url" href="http://92.27.63.80/">Bo Meson</a> for inspiration!
<hr></p>

<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%-->
<h1 class="cow-heading">Prizes!</h1>

<p>Tell us what you would do with an XLoBorg by PiBorg for a chance to win one of
three XLoBorgs.</p>

<p><a class="cow-url" href="http://www.piborg.com/">PiBorg</a>, makers of the
<a class="cow-url" href="http://www.piborg.com/xloborg">XLoBorg</a> motion and direction sensor Pi
add-on that this project uses, are offering three prizes for the best
suggestion (in 200 words or less) for interesting things to do with the XLo.
Anatomically feasible suggestions only, please :-)</p>

<p>To enter, simply email info@piborg.org with subject <em>MagPi PiBrush</em>.</p>

<p>Good luck!</p>

<p><hr></p>

<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%-->
<h1 class="cow-heading">Maker Culture meets Art</h1>

<p>There is a new conjunction emerging around open hardware, maker culture, and
art. Remember how pop culture changed with the advent of punk in the late
seventies? We seem to be witnessing a similar explosion of 'garageband'
creativity in the tennies, and the Pi is proudly leading the charge for
general purpose open source devices<span class="cow-footnote" name="footnote1"><sup><a href="#footnote1">1</a></sup></span> in this
context. (The Pi foundation even has an <em>artist in residence</em> &mdash;
<a class="cow-url" href="http://www.raspberrypi.org/archives/4329">Rachel Rayns</a>.)</p>

<p><a class="cow-url" href="http://en.wikipedia.org/wiki/List_of_open-source_hardware_projects">Open-source hardware</a> allows people to make their own robots, cameras, or even
electrocardiagraph machines by downloading schematics and incorporating any
changes they need &mdash; and, typically, free open-source software is available to
run these projects. 3D printers, laser cutters and CNC routers have helped
this adoption of the open-everything ethos<span class="cow-footnote" name="footnote2"><sup><a href="#footnote2">2</a></sup></span>.</p>

<p>There are stacks of DIY projects based on the Raspberry Pi, and the flood
showns no sign of slowing. The Pi is the first general purpose device (in
contrast to the magnificent but more specialised Arduino) which is very easy
to cobble together with add-on electronics. The thriving community that has
grown up around the Pi (including this magazine) is making a huge impact, from
changes in the <a class="cow-url" href="http://pi.gate.ac.uk/pages/schools.html">UK schools
curriculum</a> to <a class="cow-url" href="http://learn.adafruit.com/onion-pi/">the Onion Pi
anti-surveillance</a> device, to <a class="cow-url" href="http://www.dexterindustries.com/BrickPi/">BrickPi's</a> new brains for old lego Robots.</p>

<p>This article describes the <a class="cow-url" href="http://pi.gate.ac.uk/pages/pibrush.html">PiBrush</a>, a simple on-screen painting system that uses the
<a class="cow-url" href="http://www.piborg.com/xloborg">XLoBorg</a> motion and direction sensor add-on
board from <a class="cow-url" href="http://www.piborg.com/">PiBorg</a>. The XLoBorg adds an
accelerometer and a magnetometer (compass) to your Pi and makes all sorts of
motion-based interaction possible &mdash; like a
<a class="cow-url" href="http://en.wikipedia.org/wiki/Kinect">Kinect</a> but free and open. Here's a
PiBrush picture:</p>

<p><img class="cow-img" src="images/pi-brush-snap.jpg" alt="PiBrush Snapshot" width="500"></p>

<p>The PiBrush is an interactive art and technology exhibit (a rather grand name
for something so small!) that simulates flicking paint off the end of a
paintbrush onto canvas &mdash; as <a class="cow-url" href="http://en.wikipedia.org/wiki/Jackson_Pollock">Jackson Pollock</a> famously did in the 1940s and 50s. It utilizes two Raspberry
Pis (one Model B and one Model A), one <a class="cow-url" href="http://piborg.org/xloborg/">XLoBorg</a>,
a battery pack, a display, and two Wi-Fi dongles. The Model A is held by the
user and waved around. It collects acceleration data with the XLoBorg, which
it then transmits via Wi-Fi to the Model B, which processes the data
collected into paint droplets and displays them on the screen, connected via
HDMI. Functionally it looks like this:</p>

<p><img class="cow-img" src="images/pibrush/pibrush_architecture.svg" alt="PiBrush Architecture" width="500"></p>

<p>And here's the hardware (we may improve on the elastic band in future models
:-)). The Model A (client):</p>

<!--% %[-->

<!--% | %(images/pibrush/modelb.jpg, %image(images/pibrush/thumbs/modelb.jpg, Model B, 200, 150)) | %(images/pibrush/modela.jpg, %image(images/pibrush/thumbs/modela.jpg, Model A, 200, 150)) |-->

<!--% %]-->


<p><img class="cow-img" src="images/pibrush/modela.jpg" alt="Model A" width="500"></p>

<p>And the Model B (server):</p>

<p><img class="cow-img" src="images/pibrush/modelb.jpg" alt="Model B" width="500"></p>

<p>(The red button on the server saves the picture and starts another.)</p>


<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%-->

<p><a name="programming"></a></p><h1 class="cow-heading">The Code</h1>

<p>The hardware for this project is fairly straightforward. The XLoBorg plugs
directly into the GPIO pins as a block on the client, and a push button simply
connects a ground and GPIO pin on the server. The software is where the
challenge lies: intermediate to advanced code to write, using Python and some
basic physics. </p>

<p>The code is available on
<a class="cow-url" href="https://github.com/hamishcunningham/pi-tronics/tree/master/pibrush/bin">GitHub</a> and as a <a class="cow-url" href="http://pi.gate.ac.uk/pages/package.html">Rasbian package in
an Ubuntu PPA</a>. The core is in <b>accel<span class="cow-escape">_</span>client.py</b> which runs on the Model A
and <b>accel<span class="cow-escape">_</span>server.py</b> which runs on the Model B. The former reads from the
sensor and sends data across the network; the latter processes the data and
displays the output on the screen.</p>

<p>The server script on the Model B really does most of the work with handling the
physics simulation and the display. There's a lot that goes on it, but not a
lot of space here so let's focus on just the most interesting bit: the paint
brush physics. This is the code that controls what happens on the screen.
Everything else that happens in the code is to get us to this point. (It's
outlined on our web page!)</p>

<p>What we end up with are the two variables <b>GAY</b> and <b>GAZ</b>. <b>G</b>ravitational
corrected <b>A</b>cceleration in the <b>Y</b> direction and <b>G</b>ravitational corrected
<b>A</b>cceleration in the <b>Z</b> direction. They indicate horizontal and vertical
acceleration relative to the screen. It's critical information we need to
make something <em>happen</em>.</p>

<p><pre class="prettyprint lang-python linenums">
    A = numpy.linalg.norm([GAY, GAZ])
</pre>Using the <a class="cow-url" href="http://en.wikipedia.org/wiki/Pythagorean_theorem">Pythagorean
theorem</a>, <b>A</b> is the total acceleration of the Model A, with respect to the
screen, ignoring gravity. We can use this number to detect roughly if we're
speeding up or slowing down, but does that move the brush?</p>

<p><pre class="prettyprint lang-python linenums">
    if fast == 1:
        VX = VX - GAY * dt * 170
        VY = VY - GAZ * dt * 170
        BX = BX + VX * dt * 120
        BY = BY + VY * dt * 120
</pre>Assume we know we're moving quickly. This bit of code then is what actually
moves the virtual paint brush. However, we only have acceleration data, how do
we turn this into movement? Through something called <em>twice integration</em>.</p>

<p>Imagine you're in a car traveling down the motorway at 100 kph. In one hour
you will have travelled 100 km. Easy? That's integrating once, going from
velocity to displacement. Acceleration is the rate of change in velocity,
e.g. the speed at which the needle of the speedometer in the car climbs.
Now imagine accelerating at 1 kph per second. After 10 seconds you'll be going
10 kph faster, so instead of 100 kph you're now going 110 kph. Apply that to
the distance you've travelled and you have twice integration.
(Fun fact: if you kept up that acceleration, after an hour you'd be going 3,700
kph and would have traveled 36,370 km. Or almost around the Earth.)</p>

<p><img class="cow-img" src="images/pibrush/twice_integration.svg" alt="Twice Integration" width="500"></p>

<p>Similarly, we increment the brush velocity by the acceleration, factored by the
timestep (delay between loops) to keep the animation smooth. The next
integration increments the brush position by adding on the current velocity, also
factored by the timestep. Both times there are scaling factors, 170 and 120.
These are empirical coeffecients that make it look nice, but can be thought of
as saying something like 1 pixel represents 170 meters.</p>

<p><pre class="prettyprint lang-python linenums">
        if P &gt; 0:
            V = numpy.linalg.norm([VX, VY])
            S = S + V
            d = A * random.randint(3, 5) * 25 + V
</pre>Now that the brush is moving, how to determine where the paint droplets go?
<b>P</b> is the total ammount of paint on the brush. If there's paint left we can
do something with it. Vaguely you expect that the further and faster the brush
has moved the more paint droplets. <b>V</b> is combined brush velocity, and <b>S</b> is
brush displacement. <b>d</b> is our rough estimtate of paint droplet spacing.</p>

<p>Paint droplets (probably) fly off due to two factors:</p>
<ul>
<li><a class="cow-url" href="http://www.mne.psu.edu/cimbala/Learning/Fluid/Introductory/what_is_fluid_mechanics.htm">Fluid mechanics</a>. What happens when you move a glass of water too quickly.</li>
<li><a class="cow-url" href="http://en.wikipedia.org/wiki/Drag_%28physics%29">Air resistance</a>. This is
  the equivalent of wind in your hair from walking fast.</li>
</ul>

<p><img class="cow-img" src="images/pibrush/paint_factors.svg" alt="Paint Droplet Factors" width="500"></p>

<p>Both of these are rather complex subjects so I've lumped them together as they
produce similar results &mdash; paint flying off the brush. <b>d</b> is made up of <b>A</b>
times a random factor (the fluid dynamics bit), and <b>V</b> is added for the air
resistance bit. The random factor is there to take into account things like
globs of paint or hairs in the brush getting tangled. 25 is the scaling factor
this time. (Only applied to acceleration as the velocity was scaled before.)</p>

<p><pre class="prettyprint lang-python linenums">
            if S &gt; d:
                S = S - d
                # draw a droplet!
</pre>Now, if the brush has travelled further than the expected paint droplet spacing
according to our simulation, we should draw a droplet! </p>
<span class="cow-footnote-section">
<h1 class="cow-heading">Footnotes</h1>
<p><ol>
<li>
<a class="cow-footnote-anchor" name="footnote1"></a>
<span class="cow-footnote-text" name="footnote1">The Pi isn't actually <em>open
hardware</em> at present, but it does run on <em>open source</em> software.</span>
</li>
<li>
<a class="cow-footnote-anchor" name="footnote2"></a>
<span class="cow-footnote-text" name="footnote2">Two great books with the
same name document this movement: the fictional <em>Makers</em> by
<a class="cow-url" href="http://craphound.com/">Cory Doctorow</a>, and
the non-fiction <em>Makers</em> by <a class="cow-url" href="http://about.me/andersonchris">Chris
Anderson</a>.</span>
</li>
</ol></p>
</span>
<script src="https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js"></script>
</body></html>
